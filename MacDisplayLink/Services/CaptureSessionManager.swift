//
//  CaptureSessionManager.swift
//  MacDisplayLink
//
//  Created by Claude on 12/22/25.
//

import AVFoundation
import Combine
import CoreImage
import SwiftUI
import Accelerate

/// ë¹„ë””ì˜¤/ì˜¤ë””ì˜¤ ìº¡ì³ ì„¸ì…˜ ê´€ë¦¬ ì„œë¹„ìŠ¤
/// AVCaptureSessionì„ í†µí•´ ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ í”„ë ˆì„ ë° ì˜¤ë””ì˜¤ ìº¡ì³
class CaptureSessionManager: NSObject, ObservableObject {
    @Published var currentFrame: CGImage?
    @Published var hasSignal: Bool = false
    @Published var signalInfo: String = "No Signal"
    @Published var audioLevel: Float = 0.0
    @Published var isMuted: Bool = false
    @Published var hasAudioSignal: Bool = false

    private let captureSession = AVCaptureSession()
    private let videoOutput = AVCaptureVideoDataOutput()
    private let audioOutput = AVCaptureAudioDataOutput()
    private let audioPreviewOutput = AVCaptureAudioPreviewOutput()
    private let videoQueue = DispatchQueue(label: "com.echo.MacDisplayLink.videoQueue")
    private let audioQueue = DispatchQueue(label: "com.echo.MacDisplayLink.audioQueue")
    private let ciContext = CIContext()

    private var currentDevice: AVCaptureDevice?
    private var audioLevelLogCount = 0
    private var smoothedAudioLevel: Float = 0.0
    private weak var recordingManager: RecordingManager?

    init(recordingManager: RecordingManager? = nil) {
        self.recordingManager = recordingManager
        super.init()
        setupVideoOutput()
        setupAudioOutput()
        setupAudioPreviewOutput()
    }

    /// ë¹„ë””ì˜¤ ì¶œë ¥ ì„¤ì •
    private func setupVideoOutput() {
        videoOutput.setSampleBufferDelegate(self, queue: videoQueue)
        videoOutput.alwaysDiscardsLateVideoFrames = true // ì§€ì—° í”„ë ˆì„ ë²„ë¦¼

        if captureSession.canAddOutput(videoOutput) {
            captureSession.addOutput(videoOutput)
        }
    }

    /// ì˜¤ë””ì˜¤ ì¶œë ¥ ì„¤ì • (ë ˆë²¨ ì¸¡ì •ìš©)
    private func setupAudioOutput() {
        audioOutput.setSampleBufferDelegate(self, queue: audioQueue)

        if captureSession.canAddOutput(audioOutput) {
            captureSession.addOutput(audioOutput)
        }
    }

    /// ì˜¤ë””ì˜¤ í”„ë¦¬ë·° ì¶œë ¥ ì„¤ì • (ì¬ìƒìš©)
    private func setupAudioPreviewOutput() {
        // ê¸°ë³¸ ë³¼ë¥¨ ì„¤ì •
        audioPreviewOutput.volume = 1.0

        if captureSession.canAddOutput(audioPreviewOutput) {
            captureSession.addOutput(audioPreviewOutput)
        }
    }

    /// ë””ë°”ì´ìŠ¤ë¡œ ì„¸ì…˜ êµ¬ì„±
    func configureSession(with device: AVCaptureDevice) {
        videoQueue.async { [weak self] in
            guard let self = self else { return }

            // ê¸°ì¡´ ì„¸ì…˜ ì •ë¦¬
            self.captureSession.beginConfiguration()

            // ê¸°ì¡´ ì…ë ¥ ì œê±°
            self.captureSession.inputs.forEach { input in
                self.captureSession.removeInput(input)
            }

            do {
                // ë¹„ë””ì˜¤ ì…ë ¥ ì¶”ê°€
                let videoInput = try AVCaptureDeviceInput(device: device)
                if self.captureSession.canAddInput(videoInput) {
                    self.captureSession.addInput(videoInput)
                    self.currentDevice = device
                    print("âœ… [CaptureSession] ë¹„ë””ì˜¤ ì…ë ¥ ì¶”ê°€: \(device.localizedName)")
                }

                // ì˜¤ë””ì˜¤ ì…ë ¥ ì¶”ê°€ (ê°™ì€ ë””ë°”ì´ìŠ¤ì—ì„œ ì˜¤ë””ì˜¤ ì°¾ê¸°)
                if let audioDevice = self.findAudioDevice(for: device) {
                    let audioInput = try AVCaptureDeviceInput(device: audioDevice)
                    if self.captureSession.canAddInput(audioInput) {
                        self.captureSession.addInput(audioInput)
                        print("âœ… [CaptureSession] ì˜¤ë””ì˜¤ ì…ë ¥ ì¶”ê°€: \(audioDevice.localizedName)")
                    }
                } else {
                    print("âš ï¸ [CaptureSession] ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                }
            } catch {
                print("âŒ [CaptureSession] ë””ë°”ì´ìŠ¤ ì…ë ¥ ì¶”ê°€ ì‹¤íŒ¨: \(error.localizedDescription)")
                DispatchQueue.main.async {
                    self.hasSignal = false
                    self.signalInfo = "Error: \(error.localizedDescription)"
                }
            }

            self.captureSession.commitConfiguration()

            // ì„¸ì…˜ ì‹œì‘
            if !self.captureSession.isRunning {
                self.captureSession.startRunning()
                print("â–¶ï¸ [CaptureSession] ì„¸ì…˜ ì‹œì‘")
            }
        }
    }

    /// ë¹„ë””ì˜¤ ë””ë°”ì´ìŠ¤ì— ëŒ€ì‘í•˜ëŠ” ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ ì°¾ê¸°
    private func findAudioDevice(for videoDevice: AVCaptureDevice) -> AVCaptureDevice? {
        // ëª¨ë“  ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ ê²€ìƒ‰
        let deviceTypes: [AVCaptureDevice.DeviceType]
        if #available(macOS 14.0, *) {
            deviceTypes = [.external, .microphone]
        } else {
            deviceTypes = [.externalUnknown, .builtInMicrophone]
        }

        let discoverySession = AVCaptureDevice.DiscoverySession(
            deviceTypes: deviceTypes,
            mediaType: .audio,
            position: .unspecified
        )

        // ë¹„ë””ì˜¤ ë””ë°”ì´ìŠ¤ì™€ ì´ë¦„ì´ ìœ ì‚¬í•œ ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ ì°¾ê¸°
        for audioDevice in discoverySession.devices {
            print("  ğŸ” ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ ë°œê²¬: \(audioDevice.localizedName)")

            // ê°™ì€ ì œì¡°ì‚¬/ëª¨ë¸ëª…ì„ í¬í•¨í•˜ëŠ” ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ ì°¾ê¸°
            if audioDevice.localizedName.contains(videoDevice.localizedName) ||
               videoDevice.localizedName.contains(audioDevice.localizedName) ||
               audioDevice.uniqueID.contains(videoDevice.uniqueID.prefix(10)) {
                print("  âœ… ë§¤ì¹­ëœ ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤: \(audioDevice.localizedName)")
                return audioDevice
            }
        }

        // ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì™¸ë¶€ ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ ë°˜í™˜
        let externalAudioDevice = discoverySession.devices.first { device in
            !device.localizedName.lowercased().contains("built-in")
        }

        if let device = externalAudioDevice {
            print("  âš ï¸ ì²« ë²ˆì§¸ ì™¸ë¶€ ì˜¤ë””ì˜¤ ë””ë°”ì´ìŠ¤ ì‚¬ìš©: \(device.localizedName)")
        }

        return externalAudioDevice
    }

    /// ì„¸ì…˜ ì¤‘ì§€
    func stopSession() {
        videoQueue.async { [weak self] in
            guard let self = self else { return }

            if self.captureSession.isRunning {
                self.captureSession.stopRunning()
                print("â¸ [CaptureSession] ì„¸ì…˜ ì¤‘ì§€")
            }

            // ìŠ¤ë¬´ë”© ë ˆë²¨ ë¦¬ì…‹
            self.smoothedAudioLevel = 0.0

            DispatchQueue.main.async {
                self.hasSignal = false
                self.hasAudioSignal = false
                self.currentFrame = nil
                self.signalInfo = "No Signal"
                self.audioLevel = 0.0
            }
        }
    }

    /// ì‹ í˜¸ ì •ë³´ ì—…ë°ì´íŠ¸
    private func updateSignalInfo() {
        guard let device = currentDevice else {
            DispatchQueue.main.async { [weak self] in
                self?.signalInfo = "No Signal"
                self?.hasSignal = false
            }
            return
        }

        let format = device.activeFormat
        let dimensions = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
        let frameRate = Int(Int64(device.activeVideoMaxFrameDuration.timescale) / device.activeVideoMaxFrameDuration.value)

        let info = "\(dimensions.width)Ã—\(dimensions.height) @ \(frameRate)fps"

        DispatchQueue.main.async { [weak self] in
            self?.signalInfo = info
            self?.hasSignal = true
        }
    }

    /// RMS ì˜¤ë””ì˜¤ ë ˆë²¨ ê³„ì‚°
    private func calculateAudioLevel(from sampleBuffer: CMSampleBuffer) -> Float {
        // CMBlockBuffer ì§ì ‘ ì ‘ê·¼ ë°©ì‹
        guard let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) else {
            if audioLevelLogCount < 3 {
                print("âš ï¸ [AudioLevel] CMBlockBuffer ì—†ìŒ")
                audioLevelLogCount += 1
            }
            return 0.0
        }

        var totalLength: Int = 0
        var dataPointer: UnsafeMutablePointer<Int8>?

        let status = CMBlockBufferGetDataPointer(
            blockBuffer,
            atOffset: 0,
            lengthAtOffsetOut: nil,
            totalLengthOut: &totalLength,
            dataPointerOut: &dataPointer
        )

        guard status == kCMBlockBufferNoErr,
              let pointer = dataPointer,
              totalLength > 0 else {
            if audioLevelLogCount < 3 {
                print("âš ï¸ [AudioLevel] ë°ì´í„° í¬ì¸í„° ì‹¤íŒ¨: \(status)")
                audioLevelLogCount += 1
            }
            return 0.0
        }

        // formatDescriptionì—ì„œ ì˜¤ë””ì˜¤ í¬ë§· í™•ì¸
        guard let formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer) else {
            return 0.0
        }

        let streamDescription = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription)
        guard let asbd = streamDescription?.pointee else {
            return 0.0
        }

        let bytesPerSample = Int(asbd.mBitsPerChannel / 8)
        let sampleCount = totalLength / bytesPerSample

        guard sampleCount > 0 else {
            return 0.0
        }

        // ë””ë²„ê¹…: ìµœì´ˆ 1íšŒ í¬ë§· ì •ë³´ ì¶œë ¥
        if audioLevelLogCount == 0 {
            print("ğŸš [AudioFormat] SampleRate: \(asbd.mSampleRate), Channels: \(asbd.mChannelsPerFrame), BitsPerChannel: \(asbd.mBitsPerChannel)")
        }

        // Int16 ë˜ëŠ” Float32 ì²˜ë¦¬
        var rms: Float = 0.0

        if bytesPerSample == 2 { // Int16
            let samples = UnsafeBufferPointer<Int16>(
                start: UnsafeRawPointer(pointer).assumingMemoryBound(to: Int16.self),
                count: sampleCount
            )

            var sum: Float = 0.0
            var maxSample: Int16 = 0
            for sample in samples {
                let normalized = Float(sample) / Float(Int16.max)
                sum += normalized * normalized
                maxSample = max(maxSample, abs(sample))
            }

            rms = sqrt(sum / Float(samples.count))

            if audioLevelLogCount < 5 {
                print("ğŸš [AudioLevel] Int16 - MaxSample: \(maxSample), RMS: \(rms)")
            }
        } else if bytesPerSample == 4 { // Float32
            let samples = UnsafeBufferPointer<Float>(
                start: UnsafeRawPointer(pointer).assumingMemoryBound(to: Float.self),
                count: sampleCount
            )

            var sum: Float = 0.0
            var maxSample: Float = 0.0
            for sample in samples {
                sum += sample * sample
                maxSample = max(maxSample, abs(sample))
            }

            rms = sqrt(sum / Float(samples.count))

            if audioLevelLogCount < 5 {
                print("ğŸš [AudioLevel] Float32 - MaxSample: \(maxSample), RMS: \(rms)")
            }

            // Float32ê°€ ì •ê·œí™”ë˜ì§€ ì•Šì€ ê²½ìš° (maxSample > 1.0) ì •ê·œí™”
            if maxSample > 1.0 {
                rms = rms / maxSample
                if audioLevelLogCount < 5 {
                    print("ğŸš [AudioLevel] Float32 ì •ê·œí™”ë¨ - ì¡°ì •ëœ RMS: \(rms)")
                }
            }
        } else {
            if audioLevelLogCount < 3 {
                print("âš ï¸ [AudioLevel] ì§€ì›í•˜ì§€ ì•ŠëŠ” ìƒ˜í”Œ í¬ê¸°: \(bytesPerSample)")
            }
            return 0.0
        }

        if audioLevelLogCount < 5 {
            audioLevelLogCount += 1
        }

        // 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì •ê·œí™”
        guard rms > 0.00001 else { // ë§¤ìš° ì‘ì€ ê°’ì€ 0ìœ¼ë¡œ ì²˜ë¦¬
            return 0.0
        }

        // RMSë¥¼ dBë¡œ ë³€í™˜ í›„ 0.0~1.0 ë²”ìœ„ë¡œ ë§¤í•‘
        let decibels = 20 * log10(rms)

        // -60dB ~ 0dB ë²”ìœ„ë¥¼ 0.0~1.0ìœ¼ë¡œ ë§¤í•‘ (ë” ë„“ì€ ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€)
        let normalizedLevel = max(0.0, min(1.0, (decibels + 60) / 60))

        return normalizedLevel
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate & AVCaptureAudioDataOutputSampleBufferDelegate

extension CaptureSessionManager: AVCaptureVideoDataOutputSampleBufferDelegate, AVCaptureAudioDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        // ë¹„ë””ì˜¤ ì¶œë ¥ì¸ì§€ ì˜¤ë””ì˜¤ ì¶œë ¥ì¸ì§€ êµ¬ë¶„
        if output == videoOutput {
            handleVideoOutput(sampleBuffer)
        } else if output == audioOutput {
            handleAudioOutput(sampleBuffer)
        }
    }

    /// ë¹„ë””ì˜¤ ì¶œë ¥ ì²˜ë¦¬
    private func handleVideoOutput(_ sampleBuffer: CMSampleBuffer) {
        // í”„ë ˆì„ ìˆ˜ì‹  ì‹œ ì‹ í˜¸ ì •ë³´ ì—…ë°ì´íŠ¸ (ìµœì´ˆ 1íšŒ)
        if !hasSignal {
            updateSignalInfo()
        }

        // RecordingManagerì— ìƒ˜í”Œ ë²„í¼ ì „ë‹¬ (ë…¹í™” ì¤‘ì´ë©´)
        recordingManager?.appendVideoSampleBuffer(sampleBuffer)

        // CVPixelBuffer ì¶”ì¶œ
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            return
        }

        // CIImageë¡œ ë³€í™˜
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)

        // CGImageë¡œ ë³€í™˜
        guard let cgImage = ciContext.createCGImage(ciImage, from: ciImage.extent) else {
            return
        }

        // ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ UI ì—…ë°ì´íŠ¸
        DispatchQueue.main.async { [weak self] in
            self?.currentFrame = cgImage
        }
    }

    /// ì˜¤ë””ì˜¤ ì¶œë ¥ ì²˜ë¦¬
    private func handleAudioOutput(_ sampleBuffer: CMSampleBuffer) {
        // ì˜¤ë””ì˜¤ ì‹ í˜¸ í™•ì¸ (ìµœì´ˆ 1íšŒ)
        if !hasAudioSignal {
            DispatchQueue.main.async { [weak self] in
                self?.hasAudioSignal = true
                print("ğŸ¤ [CaptureSession] ì˜¤ë””ì˜¤ ì‹ í˜¸ ê°ì§€")
            }
        }

        // ë¹„ë””ì˜¤ ì‹ í˜¸ê°€ ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ ë ˆë²¨ì„ 0ìœ¼ë¡œ ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ë°©ì§€)
        guard hasSignal else {
            DispatchQueue.main.async { [weak self] in
                guard let self = self else { return }
                self.audioPreviewOutput.volume = 0.0  // ì‹ í˜¸ ì—†ì„ ë•Œ ìŒì†Œê±°
                self.audioLevel = 0.0
                self.smoothedAudioLevel = 0.0  // ìŠ¤ë¬´ë”© ë ˆë²¨ë„ ë¦¬ì…‹
            }
            return
        }

        // RecordingManagerì— ìƒ˜í”Œ ë²„í¼ ì „ë‹¬ (ë…¹í™” ì¤‘ì´ë©´)
        recordingManager?.appendAudioSampleBuffer(sampleBuffer)

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ê³„ì‚° (ì›ë³¸)
        let rawLevel = calculateAudioLevel(from: sampleBuffer)

        // ìŠ¤ë¬´ë”© ì ìš© (Exponential Moving Average)
        // smoothingFactor: 0.2 = ë¶€ë“œëŸ¬ì›€ (ëŠë¦° ë°˜ì‘), 0.5 = ì¤‘ê°„, 0.8 = ë¹ ë¥¸ ë°˜ì‘
        let smoothingFactor: Float = 0.3
        smoothedAudioLevel = smoothedAudioLevel * (1 - smoothingFactor) + rawLevel * smoothingFactor

        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }

            // ìŒì†Œê±° ìƒíƒœì— ë”°ë¼ í”„ë¦¬ë·° ë³¼ë¥¨ ì¡°ì ˆ
            self.audioPreviewOutput.volume = self.isMuted ? 0.0 : 1.0

            // UIì— ìŠ¤ë¬´ë”©ëœ ë ˆë²¨ í‘œì‹œ
            self.audioLevel = self.smoothedAudioLevel
        }
    }
}
